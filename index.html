<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LLARVA: Vision-Action Instruction Tuning Enhances Robot Learning</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>MathJax Example</title>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            <img src="./static/images/logo.png" alt="Logo" style="width: 65px; vertical-align: middle; margin-right: -10px;">
            LLARVA: Vision-Action Instruction Tuning Enhances Robot Learning
          </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=AzlUrvUAAAAJ&hl=en">Dantong Niu*</a>,</span>
            <span class="author-block">
                <a href="https://scholar.google.com/citations?hl=en&user=1_IIcds8es4C">Yuvan Sharma*</a><sup></sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=s0Fof5IAAAAJ&hl=en">Giscard Biamby</a>,
            </span>

            <span class="author-block">
              <a href="https://people.eecs.berkeley.edu/~jquenum/">Jerome Quenum</a>,
            </span>

            <span class="author-block">
              <a href="https://yutongbai.com/">Yutong Bai</a>,
            </span>

            <span class="author-block">
              <a href="https://bfshi.github.io/">Baifeng Shi</a>,
            </span>


          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell<sup>†</sup></a>,
            </span>
            <span class="author-block">
              <a href="https://roeiherz.github.io/">Roei Herzig<sup>†</sup></a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Berkeley AI Research, UC Berkeley</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
<!--              <span class="link-block">-->
<!--                <a href="https://arxiv.org/pdf/2312.17243.pdf"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="fas fa-file-pdf"></i>-->
<!--                  </span>-->
<!--                  <span>Paper</span>-->
<!--                </a>-->
<!--              </span>-->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2312.17243"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Dantong88/LLARVA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon)</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!--<section class="hero video" style="margin-top: -50px; padding-top: 0;">-->
<!--  <div class="container is-max-desktop">-->
<!--    <div class="hero-body">-->
<!--      <div class="has-text-centered">-->
<!--        <video style="width:950px" controls>-->
<!--          <source src="./static/images/master_video.mp4" type="video/mp4">-->
<!--          Your browser does not support the video tag.-->
<!--        </video>-->
<!--        <p>-->
<!--        <div class="subtitle content has-text-justified">-->
<!--        <p>&nbsp;</p>-->
<!--        <b>ADD TITLE</b>-->
<!--        <p>&nbsp;</p>-->
<!--      </p>-->
<!--      <hr>-->
<!--      </div>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->

<section class="section" style="margin-top: -50px; padding-top: 0;">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Demostrations on Real Robot</h2>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small" style="margin-top: -50px; padding-top: 0;">
  <div class="hero-body">
    <div class="container">
      <div class="video-group">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay muted loop>
            <source src="./static/real_videos/pick_left.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay muted loop>
            <source src="./static/real_videos/pick_right_raw.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay muted loop>
            <source src="./static/real_videos/pick_right_tj.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-light is-small" style="margin-top: -50px; padding-top: 0;">
  <div class="hero-body">
    <div class="container">
      <div class="video-group">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay muted loop>
            <source src="./static/real_videos/destack_left.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay muted loop>
            <source src="./static/real_videos/destack_right_raw.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay muted loop>
            <source src="./static/real_videos/destack_right_tj.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small" style="margin-top: -50px; padding-top: 0;">
  <div class="hero-body">
    <div class="container">
      <div class="video-group">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay muted loop>
            <source src="./static/real_videos/stack_left.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay muted loop>
            <source src="./static/real_videos/stack_right_raw.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay muted loop>
            <source src="./static/real_videos/stack_right_tj.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>



<h2 class="subtitle has-text-centered">
  We fine-tune LLARVA with three downstream task <strong>3 tasks</strong>: "pick blue cube", "stack the cubes" and "destack cube" on real robot.
</br>
  <strong>Left:</strong> side views from third person. <strong>Middle:</strong> raw input to LLARVA model. <strong>Right:</strong> visualization of the predicted 2-D visual traces
<hr>
</h2>



<!--######################################################################################-->
<!--this is for rlbench demos-->
<section class="section" >
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Demostrations on RLBench</h2>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small" style="margin-top: -50px; padding-top: 0;">
  <div class="hero-body">
    <div class="container">
      <div class="video-group">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay muted loop>
            <source src="./static/sim_videos/sweep_render.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay muted loop>
            <source src="./static/sim_videos/sweep_render.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay muted loop>
            <source src="./static/sim_videos/sweep_render.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small" style="margin-top: -50px; padding-top: 0;">
  <div class="hero-body">
    <div class="container">
      <div class="video-group">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay muted loop>
            <source src="./static/sim_videos/meat_off_grill_render.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay muted loop>
            <source src="./static/sim_videos/meat_off_grill_render.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay muted loop>
            <source src="./static/sim_videos/meat_off_grill_render.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small" style="margin-top: -50px; padding-top: 0;">
  <div class="hero-body">
    <div class="container">
      <div class="video-group">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay muted loop>
            <source src="./static/sim_videos/money_render.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay muted loop>
            <source src="./static/sim_videos/money_render.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay muted loop>
            <source src="./static/sim_videos/money_render.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>



<h2 class="subtitle has-text-centered">
  We evaluate LLARVA on 18 RLBench tasks, with the selected 3 tasks showing some emergent properties.
</br>
  <strong>Left:</strong> rendered 3-D videos. <strong>Middle:</strong> raw input to LLARVA model. <strong>Right:</strong> visualization of the predicted 2-D visual traces
<hr>
</h2>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            In recent years, instruction-tuned Large Multimodal Models (LMMs) have been successful at several tasks,
            including image captioning and visual question answering;
            yet leveraging these models remains an open question for robotics.
            Prior LMMs for robotics applications have been extensively trained on language and action data, but their ability to generalize in different settings has often been less than desired.
            To address this, we introduce LLARVA, a model trained with a novel instruction tuning method that leverages structured prompts to unify a range of robotic learning tasks, scenarios, and environments.
            Additionally, we show that predicting intermediate 2-D representations, which we refer to as "visual traces", can help further align vision and action spaces for robot learning.
            We generate 8.5M image-visual trace pairs from the Open X-Embodiment dataset in order to pre-train our model,
            and we evaluate on 18 different tasks in the RLBench simulator as well as a physical Franka Emika Panda 7-DoF robot.
            Our experiments yield strong performance, demonstrating that LLARVA---using 2-D and language representations---performs well compared to several contemporary baselines, and can generalize across various robot environments and configurations.
          </p>
          </p>
        </div>
      </div>
    </div>
    <hr>
  </div>
</section>

<!--######################################################################################-->
<!--this is for teaser-->

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">What is "Vision-Action Instruct Tuning"?</h2>
        <img style="width:950px" src="./static/gif/teaser_2.gif">

        <div class="subtitle content has-text-justified">
          <p>&nbsp;</p>
          We introduce a novel instruction tuning method that leverages structured prompts to unify a range of robotic learning tasks,
          scenarios, and environments and 2-D visual traces to further align vision and action spaces.
          The model works via a language instruction that contains <strong>robot model</strong>, <strong>control mode</strong>, <strong>robot task</strong>, <strong>proprioceptive information</strong>,
          and <strong>number of predicted steps</strong> , and outputs text with the next robot action(s) and the visual trace for the remainder of the episode.
          <p>&nbsp;</p>
        </div>

      </div>
    </div>
    <hr>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">The Architecture of LLARVA</h2>
        <img style="width:500px" src="./static/images/architecture.jpg">

        <div class="subtitle content has-text-justified">
          <p>&nbsp;
          <p>In our proposed pipeline, the input image undergoes processing by the frozen vision encoder \( v_{\phi}(\cdot) \), which extracts visual features and projects into a latent space via an MLP layer \( \mathcal{H} \). This aligns the visual features with the dimensionality of the language tokens. Simultaneously, the language input undergoes tokenization using a language encoder. The visual tokens and word tokens are then concatenated and fed into the auto-regressive transformers of the LMM \( f_{\theta} \), which are trained for next-token prediction.</p>

          &nbsp;</p>
        </div>

      </div>
    </div>
    <hr>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">The Vision-Action Tuning Dataset</h2>
        <img style="width:1000px" src="./static/images/dataset.jpg">

        <div class="subtitle content has-text-justified">
          <p>&nbsp;</p>
          For the pre-training of LLARVA, we generate 8.5M image-visual trace pairs from the Open X-Embodiment (OXE) dataset.
          Our dataset consists of images from a diverse collection of 37 OXE subsets with 13 different robots, including a wide assortment of tasks,
          environments, cameras (and thus images), and end-effectors, among other factors.
          For each image in an episode, we calculate the 2-D visual trace of the end-effector \( \mathcal{P}_{t:N} \).
          For this purpose, we use a bounding box detector that is trained specifically on each of the different end-effectors in OXE.
        </div>

      </div>
    </div>
    <hr>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">The Visualization of the 2-D Visual Trace</h2>
        <img style="width:1000px" src="./static/images/visual_trace.jpg">

        <div class="subtitle content has-text-justified">
          <p>&nbsp;</p>
          We show visual traces that our model can plan alternative yet correct paths.
          For example, in the top left image,  the gripper takes a different path (go left) than the ground truth (go right), yet it still succeeds in destacking the cube.
          The above figure also shows qualitatively that the visual trace can help with long-horizon tasks by acting as a memory buffer that compensates for the limited number of previous robotic states the model can handle.
          For instance, in the bottom right, the task is "push maroon button, then push green button". It can be seen that the visual trace helps the model reach the "green button" after finishing the first subtask.
        </div>

      </div>
    </div>
    <hr>
  </div>
</section>








<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{TODO,
      title={TODO},
      author={TODO},
      year={2024},
      eprint={2312.17243},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}</code></pre>
  </div>
</section>



<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/xxxx.xxxxx.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/Dantong88/LLARVA" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Acknowledgement: ?? were funded by DoD including DARPA LwLL and the Berkeley AI Research Commons. A portion of this website is built upon the codes provided by the <a href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
